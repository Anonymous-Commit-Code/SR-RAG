#!/usr/bin/env python3
"""
SR-RAG æ•°æ®åˆæˆæµæ°´çº¿
æŒ‰ç…§è®ºæ–‡æè¿°å®ç°"generation-review"æ•°æ®åˆæˆè¿‡ç¨‹
"""

import asyncio
import argparse
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(".")

from src.data.synthesis_pipeline import DataSynthesizer, SynthesisConfig
from src.data.convert_to_llamafactory import convert_all_datasets, create_dataset_info


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="SR-RAGæ•°æ®åˆæˆæµæ°´çº¿")
    
    parser.add_argument(
        "--num_samples", 
        type=int, 
        default=1000,
        help="æ¯ä¸ªä»»åŠ¡ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 1000)"
    )
    
    parser.add_argument(
        "--generator_model",
        type=str,
        default="deepseek-r1",
        help="ç”Ÿæˆå™¨æ¨¡å‹åç§° (é»˜è®¤: deepseek-r1)"
    )
    
    parser.add_argument(
        "--jury_models",
        nargs="+",
        default=["qwen", "llama", "deepseek-v3"],
        help="è¯„å®¡å›¢æ¨¡å‹åˆ—è¡¨ (é»˜è®¤: qwen llama deepseek-v3)"
    )
    
    parser.add_argument(
        "--agreement_threshold",
        type=float,
        default=0.6,
        help="è¯„å®¡å›¢é€šè¿‡é˜ˆå€¼ (é»˜è®¤: 0.6)"
    )
    
    parser.add_argument(
        "--max_attempts",
        type=int,
        default=3,
        help="å•ä¸ªæ ·æœ¬æœ€å¤§ç”Ÿæˆå°è¯•æ¬¡æ•° (é»˜è®¤: 3)"
    )
    
    parser.add_argument(
        "--convert_only",
        action="store_true",
        help="ä»…è½¬æ¢å·²æœ‰æ•°æ®ä¸ºLLaMA Factoryæ ¼å¼"
    )
    
    parser.add_argument(
        "--task",
        choices=["refinement", "classification", "rewriting", "all"],
        default="all",
        help="æŒ‡å®šè¦åˆæˆçš„ä»»åŠ¡ç±»å‹ (é»˜è®¤: all)"
    )
    
    return parser.parse_args()


async def run_synthesis(args):
    """è¿è¡Œæ•°æ®åˆæˆæµç¨‹"""
    print("=== SR-RAG æ•°æ®åˆæˆæµæ°´çº¿ ===")
    print(f"ç”Ÿæˆå™¨æ¨¡å‹: {args.generator_model}")
    print(f"è¯„å®¡å›¢æ¨¡å‹: {args.jury_models}")
    print(f"æ¯ä¸ªä»»åŠ¡æ ·æœ¬æ•°: {args.num_samples}")
    print(f"è¯„å®¡é€šè¿‡é˜ˆå€¼: {args.agreement_threshold}")
    print("="*50)
    
    # é…ç½®åˆæˆå‚æ•°
    config = SynthesisConfig(
        generator_model=args.generator_model,
        jury_models=args.jury_models,
        num_jury_members=min(3, len(args.jury_models)),
        agreement_threshold=args.agreement_threshold,
        max_synthesis_attempts=args.max_attempts,
        batch_size=10
    )
    
    # åˆ›å»ºåˆæˆå™¨
    synthesizer = DataSynthesizer(config)
    
    # æ ¹æ®æŒ‡å®šä»»åŠ¡è¿›è¡Œåˆæˆ
    if args.task == "all":
        print("å¼€å§‹åˆæˆæ‰€æœ‰ä»»åŠ¡çš„æ•°æ®é›†...")
        await synthesizer.synthesize_all_datasets(args.num_samples)
    
    elif args.task == "refinement":
        print("å¼€å§‹åˆæˆéœ€æ±‚ç»†åŒ–æ•°æ®é›†...")
        dataset = await synthesizer.synthesize_refinement_dataset(args.num_samples)
        if dataset:
            synthesizer.save_dataset(dataset, "refinement_data.json")
            print(f"æˆåŠŸåˆæˆ {len(dataset)} ä¸ªéœ€æ±‚ç»†åŒ–æ ·æœ¬")
    
    elif args.task == "classification":
        print("å¼€å§‹åˆæˆéœ€æ±‚åˆ†ç±»æ•°æ®é›†...")
        dataset = await synthesizer.synthesize_classification_dataset(args.num_samples)
        if dataset:
            synthesizer.save_dataset(dataset, "classification_data.json")
            print(f"æˆåŠŸåˆæˆ {len(dataset)} ä¸ªéœ€æ±‚åˆ†ç±»æ ·æœ¬")
    
    elif args.task == "rewriting":
        print("å¼€å§‹åˆæˆå‡†åˆ™é‡å†™æ•°æ®é›†...")
        dataset = await synthesizer.synthesize_rewriting_dataset(args.num_samples)
        if dataset:
            synthesizer.save_dataset(dataset, "rewriting_data.json")
            print(f"æˆåŠŸåˆæˆ {len(dataset)} ä¸ªå‡†åˆ™é‡å†™æ ·æœ¬")


def run_conversion():
    """è¿è¡Œæ•°æ®æ ¼å¼è½¬æ¢"""
    print("=== æ•°æ®æ ¼å¼è½¬æ¢ ===")
    print("å°†åˆæˆæ•°æ®è½¬æ¢ä¸ºLLaMA Factoryæ ¼å¼...")
    
    try:
        convert_all_datasets()
        create_dataset_info()
        print("âœ… æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥: {e}")


def check_prerequisites():
    """æ£€æŸ¥è¿è¡Œå‰ææ¡ä»¶"""
    required_files = [
        "datasets/database.json",
        "datasets/testset/gt.json"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®æ–‡ä»¶:")
        for file_path in missing_files:
            print(f"  - {file_path}")
        print("\nè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨åé‡æ–°è¿è¡Œã€‚")
        return False
    
    return True


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # æ£€æŸ¥å‰ææ¡ä»¶
    if not check_prerequisites():
        sys.exit(1)
    
    try:
        if args.convert_only:
            # ä»…è¿è¡Œæ ¼å¼è½¬æ¢
            run_conversion()
        else:
            # è¿è¡Œå®Œæ•´æµæ°´çº¿
            await run_synthesis(args)
            
            # è‡ªåŠ¨è¿è¡Œæ ¼å¼è½¬æ¢
            print("\n" + "="*50)
            run_conversion()
            
        print("\nğŸ‰ æ•°æ®åˆæˆæµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - datasets/training_data/     # åŸå§‹åˆæˆæ•°æ®")
        print("  - datasets/llamafactory_data/ # LLaMA Factoryæ ¼å¼æ•°æ®")
        print("\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨fine-tuningé…ç½®æ–‡ä»¶è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 